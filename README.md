# Distributed Training Tutorial

This repository contains theory and hands-on materials for the tutorial on Distributed Training, covering key concepts and practical exercises using PyTorch Distributed Data Parallel (DDP) and DeepSpeed for scalable AI/LLM training.

## Topics Covered
1. **Distributed Training Concepts**:
   - Vertical vs. Horizontal Scaling
   - Data, Model, and Pipeline Parallelism
2. **Hands-On**:
   - Multi-GPU Training with PyTorch DDP
   - Scalable AI/LLM Training with DeepSpeed
   - Using SLURM for Job Scheduling

## Prerequisites
- Python 3.9+
- NVIDIA GPUs and CUDA toolkit
- PyTorch, DeepSpeed, and SLURM installed

## Setup Instructions
1. Clone the repository:
   ```bash
   git clone https://github.com/sowmyashree04/distributed_training.git
   cd distributed-training-workshop
